{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline LightGBM Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data from s3 buckets\n",
    "s3 = boto3.client(\"s3\")\n",
    "data_bucket = 'ads-508-group-6-final'\n",
    "\n",
    "data_path = \"data.csv\"\n",
    "\n",
    "# downloading the test data from data_bucket\n",
    "s3.download_file(data_bucket, 'churn_model_data/train/data.csv', data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThe test dataset contains 110673 examples and 1093 columns.\u001b[0m\n",
      "\n",
      "\u001b[1mThe first 5 observations of the data: \u001b[0m \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_1083</th>\n",
       "      <th>Feature_1084</th>\n",
       "      <th>Feature_1085</th>\n",
       "      <th>Feature_1086</th>\n",
       "      <th>Feature_1087</th>\n",
       "      <th>Feature_1088</th>\n",
       "      <th>Feature_1089</th>\n",
       "      <th>Feature_1090</th>\n",
       "      <th>Feature_1091</th>\n",
       "      <th>Feature_1092</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1083.000000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>1085.000000</td>\n",
       "      <td>1086.000000</td>\n",
       "      <td>1087.000000</td>\n",
       "      <td>1088.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1090.000000</td>\n",
       "      <td>1091.000000</td>\n",
       "      <td>1092.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101060</td>\n",
       "      <td>0.367112</td>\n",
       "      <td>-0.222540</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.313837</td>\n",
       "      <td>0.019701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.291667</td>\n",
       "      <td>-0.297297</td>\n",
       "      <td>-0.390507</td>\n",
       "      <td>-1.044013</td>\n",
       "      <td>-0.440974</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.343278</td>\n",
       "      <td>-0.157604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.432432</td>\n",
       "      <td>-0.385317</td>\n",
       "      <td>0.477752</td>\n",
       "      <td>10.702828</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.304122</td>\n",
       "      <td>0.409771</td>\n",
       "      <td>5.254663</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>5.594595</td>\n",
       "      <td>5.053682</td>\n",
       "      <td>-0.206079</td>\n",
       "      <td>4.261801</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>4.226595</td>\n",
       "      <td>-0.144095</td>\n",
       "      <td>4.060302</td>\n",
       "      <td>-0.425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1093 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target  Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0     0.0        1.0        2.0        3.0        4.0        5.0        6.0   \n",
       "1     1.0        1.0        1.0        0.0        1.0        0.0        0.0   \n",
       "2     1.0        1.0        1.0        0.0        1.0        0.0        0.0   \n",
       "3     1.0        0.0        0.0        0.0        1.0        0.0        0.0   \n",
       "4     0.0        1.0        1.0        0.0        1.0        0.0        0.0   \n",
       "\n",
       "   Feature_7  Feature_8  Feature_9  ...  Feature_1083  Feature_1084  \\\n",
       "0        7.0        8.0        9.0  ...   1083.000000   1084.000000   \n",
       "1        0.0        0.0        0.0  ...      0.458333      0.000000   \n",
       "2        0.0        0.0        0.0  ...      1.291667     -0.297297   \n",
       "3        0.0        0.0        1.0  ...      0.333333     -0.432432   \n",
       "4        0.0        0.0        0.0  ...      0.458333      5.594595   \n",
       "\n",
       "   Feature_1085  Feature_1086  Feature_1087  Feature_1088  Feature_1089  \\\n",
       "0   1085.000000   1086.000000   1087.000000   1088.000000   1089.000000   \n",
       "1      0.101060      0.367112     -0.222540     -0.333333     -0.313837   \n",
       "2     -0.390507     -1.044013     -0.440974     -0.333333     -0.343278   \n",
       "3     -0.385317      0.477752     10.702828      0.166667      0.304122   \n",
       "4      5.053682     -0.206079      4.261801      5.166667      4.226595   \n",
       "\n",
       "   Feature_1090  Feature_1091  Feature_1092  \n",
       "0   1090.000000   1091.000000      1092.000  \n",
       "1      0.019701      0.000000         0.450  \n",
       "2     -0.157604      0.000000         1.000  \n",
       "3      0.409771      5.254663         0.400  \n",
       "4     -0.144095      4.060302        -0.425  \n",
       "\n",
       "[5 rows x 1093 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newline, bold, unbold = '\\n', '\\033[1m', '\\033[0m'\n",
    "\n",
    "# read the data\n",
    "test_data = pd.read_csv(data_path, header=None)\n",
    "test_data.columns = ['Target'] + [f\"Feature_{i}\" for i in range(1, test_data.shape[1])]\n",
    "\n",
    "num_examples, num_columns = test_data.shape\n",
    "print(f\"{bold}The test dataset contains {num_examples} examples and {num_columns} columns.{unbold}\\n\")\n",
    "\n",
    "# prepare the ground truth target and predicting features to send into the endpoint.\n",
    "ground_truth_label, features = test_data.iloc[:, :1], test_data.iloc[:, 1:]\n",
    "\n",
    "print(f\"{bold}The first 5 observations of the data: {unbold} \\n\")\n",
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quering Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Backend scripts have been updated in February '22 to standardize response format of endpoint response.Previous endpoints may not support verbose response type used in this notebook.To use this notebook, please launch the endpoint again. Error: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body. See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/jumpstart-ftc-lgb-classification-model in account 765790021581 for more information..",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4a004fca7a85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mquery_response_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4a004fca7a85>\u001b[0m in \u001b[0;36mquery_endpoint\u001b[0;34m(encoded_tabular_data)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'runtime.sagemaker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mContentType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoded_tabular_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body. See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/jumpstart-ftc-lgb-classification-model in account 765790021581 for more information.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4a004fca7a85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'Error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Code'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ModelError'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             raise Exception(\n\u001b[0;32m---> 22\u001b[0;31m                  \u001b[0;34m\"Backend scripts have been updated in February '22 to standardize response \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                  \u001b[0;34m\"format of endpoint response.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                  \u001b[0;34m\"Previous endpoints may not support verbose response type used in this notebook.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Backend scripts have been updated in February '22 to standardize response format of endpoint response.Previous endpoints may not support verbose response type used in this notebook.To use this notebook, please launch the endpoint again. Error: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body. See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/jumpstart-ftc-lgb-classification-model in account 765790021581 for more information.."
     ]
    }
   ],
   "source": [
    "content_type = \"text/csv\"\n",
    "def query_endpoint(encoded_tabular_data):\n",
    "    endpoint_name = 'jumpstart-ftc-lgb-classification-model'\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType=content_type, Body=encoded_tabular_data)\n",
    "    return response\n",
    "\n",
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    predicted_probabilities = np.array(model_predictions['probabilities'])\n",
    "    return predicted_probabilities\n",
    "\n",
    "# split the test data into smaller size of batches to query the endpoint due to the large size of test data. \n",
    "batch_size = 1500\n",
    "predict_prob = []\n",
    "for i in np.arange(0, num_examples, step=batch_size):\n",
    "    try:\n",
    "        query_response_batch = query_endpoint(features.iloc[i:(i+batch_size), :].to_csv(header=False, index=False).encode(\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        if e.response is not None and 'Error' in e.response and e.response.get('Error').get('Code') == 'ModelError':\n",
    "            raise Exception(\n",
    "                 \"Backend scripts have been updated in February '22 to standardize response \"\n",
    "                 \"format of endpoint response.\"\n",
    "                 \"Previous endpoints may not support verbose response type used in this notebook.\"\n",
    "                 f\"To use this notebook, please launch the endpoint again. Error: {e}.\"\n",
    "            )\n",
    "        else:\n",
    "            raise\n",
    "    try:\n",
    "        predict_prob_batch = parse_response(query_response_batch)  # prediction probability per batch\n",
    "    except (TypeError, KeyError) as e:\n",
    "        raise Exception(\n",
    "              \"Backend scripts have been updated in February '22 to standardize response \"\n",
    "              \"format of endpoint response.\"\n",
    "               \"Response from previous endpoints may not be consistent with this notebook.\"\n",
    "               f\"To use this notebook, please launch the endpoint again. Error: {e}.\"\n",
    "       )\n",
    "    predict_prob.append(predict_prob_batch)\n",
    "    \n",
    "\n",
    "predict_prob = np.concatenate(predict_prob, axis=0)\n",
    "predict_label = np.argmax(predict_prob, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So this is either due to Amazon not updating associated scripts due to update or dimensionality of training array. Regardless, this leaves us to implement XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the predictions results by plotting the confusion matrix.\n",
    "conf_matrix = confusion_matrix(y_true=ground_truth_label.values, y_pred=predict_label)\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    " \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the prediction results quantitatively.\n",
    "eval_accuracy = accuracy_score(ground_truth_label.values, predict_label)\n",
    "eval_f1_macro = f1_score(ground_truth_label.values, predict_label, average=\"macro\")\n",
    "eval_f1_micro = f1_score(ground_truth_label.values, predict_label, average=\"micro\")\n",
    "\n",
    "print (\n",
    "    f\"{bold}Evaluation result on test data{unbold}:{newline}\"\n",
    "    f\"{bold}{accuracy_score.__name__}{unbold}: {eval_accuracy}{newline}\"\n",
    "    f\"{bold}F1 Macro{unbold}: {eval_f1_macro}{newline}\"\n",
    "    f\"{bold}F1 Micro{unbold}: {eval_f1_micro}{newline}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data from s3 buckets\n",
    "s3 = boto3.client(\"s3\")\n",
    "data_bucket = 'ads-508-group-6-final'\n",
    "\n",
    "data_path = \"data.csv\"\n",
    "\n",
    "# downloading the test data from data_bucket\n",
    "s3.download_file(data_bucket, 'churn_model_data/validation/data.csv', data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newline, bold, unbold = '\\n', '\\033[1m', '\\033[0m'\n",
    "\n",
    "# read the data\n",
    "test_data = pd.read_csv(data_path, header=None)\n",
    "test_data.columns = ['Target'] + [f\"Feature_{i}\" for i in range(1, test_data.shape[1])]\n",
    "\n",
    "num_examples, num_columns = test_data.shape\n",
    "print(f\"{bold}The test dataset contains {num_examples} examples and {num_columns} columns.{unbold}\\n\")\n",
    "\n",
    "# prepare the ground truth target and predicting features to send into the endpoint.\n",
    "ground_truth_label, features = test_data.iloc[:, :1], test_data.iloc[:, 1:]\n",
    "\n",
    "print(f\"{bold}The first 5 observations of the data: {unbold} \\n\")\n",
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quering Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_type = \"text/csv\"\n",
    "def query_endpoint(encoded_tabular_data):\n",
    "    endpoint_name = 'jumpstart-ftc-lgb-classification-model'\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType=content_type, Body=encoded_tabular_data)\n",
    "    return response\n",
    "\n",
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    predicted_probabilities = np.array(model_predictions['probabilities'])\n",
    "    return predicted_probabilities\n",
    "\n",
    "# split the test data into smaller size of batches to query the endpoint due to the large size of test data. \n",
    "batch_size = 1500\n",
    "predict_prob = []\n",
    "for i in np.arange(0, num_examples, step=batch_size):\n",
    "    try:\n",
    "        query_response_batch = query_endpoint(features.iloc[i:(i+batch_size), :].to_csv(header=False, index=False).encode(\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        if e.response is not None and 'Error' in e.response and e.response.get('Error').get('Code') == 'ModelError':\n",
    "            raise Exception(\n",
    "                 \"Backend scripts have been updated in February '22 to standardize response \"\n",
    "                 \"format of endpoint response.\"\n",
    "                 \"Previous endpoints may not support verbose response type used in this notebook.\"\n",
    "                 f\"To use this notebook, please launch the endpoint again. Error: {e}.\"\n",
    "            )\n",
    "        else:\n",
    "            raise\n",
    "    try:\n",
    "        predict_prob_batch = parse_response(query_response_batch)  # prediction probability per batch\n",
    "    except (TypeError, KeyError) as e:\n",
    "        raise Exception(\n",
    "              \"Backend scripts have been updated in February '22 to standardize response \"\n",
    "              \"format of endpoint response.\"\n",
    "               \"Response from previous endpoints may not be consistent with this notebook.\"\n",
    "               f\"To use this notebook, please launch the endpoint again. Error: {e}.\"\n",
    "       )\n",
    "    predict_prob.append(predict_prob_batch)\n",
    "    \n",
    "\n",
    "predict_prob = np.concatenate(predict_prob, axis=0)\n",
    "predict_label = np.argmax(predict_prob, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the predictions results by plotting the confusion matrix.\n",
    "conf_matrix = confusion_matrix(y_true=ground_truth_label.values, y_pred=predict_label)\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    " \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the prediction results quantitatively.\n",
    "eval_accuracy = accuracy_score(ground_truth_label.values, predict_label)\n",
    "eval_f1_macro = f1_score(ground_truth_label.values, predict_label, average=\"macro\")\n",
    "eval_f1_micro = f1_score(ground_truth_label.values, predict_label, average=\"micro\")\n",
    "\n",
    "print (\n",
    "    f\"{bold}Evaluation result on test data{unbold}:{newline}\"\n",
    "    f\"{bold}{accuracy_score.__name__}{unbold}: {eval_accuracy}{newline}\"\n",
    "    f\"{bold}F1 Macro{unbold}: {eval_f1_macro}{newline}\"\n",
    "    f\"{bold}F1 Micro{unbold}: {eval_f1_micro}{newline}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data from s3 buckets\n",
    "s3 = boto3.client(\"s3\")\n",
    "data_bucket = 'ads-508-group-6-final'\n",
    "\n",
    "data_path = \"data.csv\"\n",
    "\n",
    "# downloading the test data from data_bucket\n",
    "s3.download_file(data_bucket, 'churn_model_data/test/data.csv', data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "newline, bold, unbold = '\\n', '\\033[1m', '\\033[0m'\n",
    "\n",
    "# read the data\n",
    "test_data = pd.read_csv(data_path, header=None)\n",
    "test_data.columns = ['Target'] + [f\"Feature_{i}\" for i in range(1, test_data.shape[1])]\n",
    "\n",
    "num_examples, num_columns = test_data.shape\n",
    "print(f\"{bold}The test dataset contains {num_examples} examples and {num_columns} columns.{unbold}\\n\")\n",
    "\n",
    "# prepare the ground truth target and predicting features to send into the endpoint.\n",
    "ground_truth_label, features = test_data.iloc[:, :1], test_data.iloc[:, 1:]\n",
    "\n",
    "print(f\"{bold}The first 5 observations of the data: {unbold} \\n\")\n",
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quering Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "content_type = \"text/csv\"\n",
    "def query_endpoint(encoded_tabular_data):\n",
    "    endpoint_name = 'jumpstart-ftc-lgb-classification-model'\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType=content_type, Body=encoded_tabular_data)\n",
    "    return response\n",
    "\n",
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    predicted_probabilities = np.array(model_predictions['probabilities'])\n",
    "    return predicted_probabilities\n",
    "\n",
    "# split the test data into smaller size of batches to query the endpoint due to the large size of test data. \n",
    "batch_size = 1500\n",
    "predict_prob = []\n",
    "for i in np.arange(0, num_examples, step=batch_size):\n",
    "    try:\n",
    "        query_response_batch = query_endpoint(features.iloc[i:(i+batch_size), :].to_csv(header=False, index=False).encode(\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        if e.response is not None and 'Error' in e.response and e.response.get('Error').get('Code') == 'ModelError':\n",
    "            raise Exception(\n",
    "                 \"Backend scripts have been updated in February '22 to standardize response \"\n",
    "                 \"format of endpoint response.\"\n",
    "                 \"Previous endpoints may not support verbose response type used in this notebook.\"\n",
    "                 f\"To use this notebook, please launch the endpoint again. Error: {e}.\"\n",
    "            )\n",
    "        else:\n",
    "            raise\n",
    "    try:\n",
    "        predict_prob_batch = parse_response(query_response_batch)  # prediction probability per batch\n",
    "    except (TypeError, KeyError) as e:\n",
    "        raise Exception(\n",
    "              \"Backend scripts have been updated in February '22 to standardize response \"\n",
    "              \"format of endpoint response.\"\n",
    "               \"Response from previous endpoints may not be consistent with this notebook.\"\n",
    "               f\"To use this notebook, please launch the endpoint again. Error: {e}.\"\n",
    "       )\n",
    "    predict_prob.append(predict_prob_batch)\n",
    "    \n",
    "\n",
    "predict_prob = np.concatenate(predict_prob, axis=0)\n",
    "predict_label = np.argmax(predict_prob, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the predictions results by plotting the confusion matrix.\n",
    "conf_matrix = confusion_matrix(y_true=ground_truth_label.values, y_pred=predict_label)\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    " \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Measure the prediction results quantitatively.\n",
    "eval_accuracy = accuracy_score(ground_truth_label.values, predict_label)\n",
    "eval_f1_macro = f1_score(ground_truth_label.values, predict_label, average=\"macro\")\n",
    "eval_f1_micro = f1_score(ground_truth_label.values, predict_label, average=\"micro\")\n",
    "\n",
    "print (\n",
    "    f\"{bold}Evaluation result on test data{unbold}:{newline}\"\n",
    "    f\"{bold}{accuracy_score.__name__}{unbold}: {eval_accuracy}{newline}\"\n",
    "    f\"{bold}F1 Macro{unbold}: {eval_f1_macro}{newline}\"\n",
    "    f\"{bold}F1 Micro{unbold}: {eval_f1_micro}{newline}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
